---
# Post-Install Infrastructure Playbook
# Deploys critical infrastructure components in order with health checks
# This ensures ArgoCD has all dependencies before being deployed

- name: Deploy Critical Infrastructure Components
  hosts: kube_control_plane[0]
  gather_facts: false
  become: false
  vars:
    kubectl_bin: kubectl
    kubeconfig_path: "{{ ansible_env.HOME }}/.kube/config"
    cilium_version: "1.15.8"
    multus_version: "v4.0.2"
    nginx_ingress_version: "v1.8.4"
    cert_manager_version: "v1.13.2"
    vault_version: "0.26.1"
    argocd_version: "stable"
    
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
    
  tasks:
    # ===========================================
    # PHASE 1: CNI DEPLOYMENT (CILIUM)
    # ===========================================
    - name: Phase 1 - Deploy Cilium CNI
      block:
        - name: Check if Cilium is already installed
          k8s_info:
            api_version: apps/v1
            kind: DaemonSet
            name: cilium
            namespace: kube-system
          register: cilium_check
          
        - name: Add Cilium Helm repository
          kubernetes.core.helm_repository:
            name: cilium
            repo_url: https://helm.cilium.io/
          when: cilium_check.resources | length == 0
          
        - name: Deploy Cilium CNI
          kubernetes.core.helm:
            name: cilium
            chart_ref: cilium/cilium
            release_namespace: kube-system
            create_namespace: true
            chart_version: "{{ cilium_version }}"
            values:
              cluster:
                name: "{{ cluster_name | default('pn-cluster') }}"
                id: 1
              ipam:
                mode: kubernetes
              kubeProxyReplacement: true
              k8sServiceHost: "{{ apiserver_loadbalancer_domain_name }}"
              k8sServicePort: "{{ kube_apiserver_port }}"
              hubble:
                enabled: true
                ui:
                  enabled: true
              operator:
                replicas: 2
              bpf:
                masquerade: true
              cgroup:
                autoMount:
                  enabled: true
                hostRoot: /run/cilium/cgroupv2
          when: cilium_check.resources | length == 0
          
        - name: Wait for Cilium DaemonSet to be ready
          k8s_info:
            api_version: apps/v1
            kind: DaemonSet
            name: cilium
            namespace: kube-system
            wait: true
            wait_condition:
              type: Ready
              status: "True"
            wait_timeout: 300
            
        - name: Verify Cilium pods are running on all nodes
          k8s_info:
            api_version: v1
            kind: Pod
            namespace: kube-system
            label_selectors:
              - "k8s-app=cilium"
            field_selectors:
              - "status.phase=Running"
          register: cilium_pods
          until: cilium_pods.resources | length == groups['k8s_cluster'] | length
          retries: 30
          delay: 10
          
        - name: Test pod-to-pod connectivity (deploy test pod)
          k8s:
            definition:
              apiVersion: v1
              kind: Pod
              metadata:
                name: cilium-connectivity-test
                namespace: default
              spec:
                containers:
                - name: test
                  image: busybox:1.35
                  command: ["sleep", "3600"]
            wait: true
            wait_condition:
              type: Ready
              status: "True"
            wait_timeout: 120
            
        - name: Verify CNI connectivity test
          k8s_exec:
            namespace: default
            pod: cilium-connectivity-test
            command: nslookup kubernetes.default.svc.cluster.local
          register: connectivity_test
          failed_when: "'can't resolve' in connectivity_test.stdout"
          
        - name: Clean up connectivity test pod
          k8s:
            api_version: v1
            kind: Pod
            name: cilium-connectivity-test
            namespace: default
            state: absent
            
        - name: Phase 1 Complete - Cilium CNI is healthy
          debug:
            msg: "‚úÖ Cilium CNI deployed and verified healthy"

    # ===========================================
    # PHASE 2: MULTUS (MULTI-NETWORKING)
    # ===========================================
    - name: Phase 2 - Deploy Multus CNI
      block:
        - name: Check if Multus is already installed
          k8s_info:
            api_version: apps/v1
            kind: DaemonSet
            name: kube-multus-ds
            namespace: kube-system
          register: multus_check
          
        - name: Deploy Multus CNI
          k8s:
            definition:
              apiVersion: v1
              kind: ConfigMap
              metadata:
                name: multus-cni-config
                namespace: kube-system
              data:
                cni-conf.json: |
                  {
                    "name": "multus-cni-network",
                    "type": "multus",
                    "cniVersion": "0.3.1",
                    "delegates": [],
                    "kubeconfig": "/etc/cni/net.d/multus.d/multus.kubeconfig"
                  }
          when: multus_check.resources | length == 0
          
        - name: Deploy Multus DaemonSet
          k8s:
            definition: "{{ lookup('url', 'https://raw.githubusercontent.com/k8snetworkplumbingwg/multus-cni/' + multus_version + '/deployments/multus-daemonset-thick.yml', split_lines=False) | from_yaml_all | list }}"
          when: multus_check.resources | length == 0
          
        - name: Wait for Multus DaemonSet to be ready
          k8s_info:
            api_version: apps/v1
            kind: DaemonSet
            name: kube-multus-ds
            namespace: kube-system
            wait: true
            wait_condition:
              type: Ready
              status: "True"
            wait_timeout: 180
            
        - name: Phase 2 Complete - Multus CNI is healthy
          debug:
            msg: "‚úÖ Multus CNI deployed and verified healthy"

    # ===========================================
    # PHASE 3: INGRESS-NGINX
    # ===========================================
    - name: Phase 3 - Deploy NGINX Ingress Controller
      block:
        - name: Check if NGINX Ingress is already installed
          k8s_info:
            api_version: apps/v1
            kind: Deployment
            name: ingress-nginx-controller
            namespace: ingress-nginx
          register: nginx_check
          
        - name: Add NGINX Ingress Helm repository
          kubernetes.core.helm_repository:
            name: ingress-nginx
            repo_url: https://kubernetes.github.io/ingress-nginx
          when: nginx_check.resources | length == 0
          
        - name: Deploy NGINX Ingress Controller
          kubernetes.core.helm:
            name: ingress-nginx
            chart_ref: ingress-nginx/ingress-nginx
            release_namespace: ingress-nginx
            create_namespace: true
            chart_version: "4.8.3"
            values:
              controller:
                replicaCount: 2
                service:
                  type: NodePort
                  nodePorts:
                    http: 30080
                    https: 30443
                nodeSelector:
                  kubernetes.io/os: linux
                tolerations:
                  - key: node-role.kubernetes.io/control-plane
                    operator: Equal
                    effect: NoSchedule
                config:
                  use-forwarded-headers: "true"
                  compute-full-forwarded-for: "true"
                  use-proxy-protocol: "false"
                metrics:
                  enabled: true
                  serviceMonitor:
                    enabled: false
          when: nginx_check.resources | length == 0
          
        - name: Wait for NGINX Ingress Controller to be ready
          k8s_info:
            api_version: apps/v1
            kind: Deployment
            name: ingress-nginx-controller
            namespace: ingress-nginx
            wait: true
            wait_condition:
              type: Available
              status: "True"
            wait_timeout: 300
            
        - name: Test NGINX Ingress with health check
          uri:
            url: "http://{{ ansible_default_ipv4.address }}:30080/healthz"
            method: GET
            return_content: true
          register: nginx_health
          until: nginx_health.status == 200
          retries: 10
          delay: 15
          
        - name: Phase 3 Complete - NGINX Ingress is healthy
          debug:
            msg: "‚úÖ NGINX Ingress Controller deployed and verified healthy"

    # ===========================================
    # PHASE 4: CERT-MANAGER
    # ===========================================
    - name: Phase 4 - Deploy Cert-Manager
      block:
        - name: Check if Cert-Manager is already installed
          k8s_info:
            api_version: apps/v1
            kind: Deployment
            name: cert-manager
            namespace: cert-manager
          register: certmanager_check
          
        - name: Add Cert-Manager Helm repository
          kubernetes.core.helm_repository:
            name: jetstack
            repo_url: https://charts.jetstack.io
          when: certmanager_check.resources | length == 0
          
        - name: Deploy Cert-Manager CRDs
          k8s:
            definition: "{{ lookup('url', 'https://github.com/cert-manager/cert-manager/releases/download/' + cert_manager_version + '/cert-manager.crds.yaml', split_lines=False) | from_yaml_all | list }}"
          when: certmanager_check.resources | length == 0
          
        - name: Deploy Cert-Manager
          kubernetes.core.helm:
            name: cert-manager
            chart_ref: jetstack/cert-manager
            release_namespace: cert-manager
            create_namespace: true
            chart_version: "{{ cert_manager_version }}"
            values:
              installCRDs: false  # We installed them manually above
              replicaCount: 2
              nodeSelector:
                kubernetes.io/os: linux
              tolerations:
                - key: node-role.kubernetes.io/control-plane
                  operator: Equal
                  effect: NoSchedule
          when: certmanager_check.resources | length == 0
          
        - name: Wait for Cert-Manager to be ready
          k8s_info:
            api_version: apps/v1
            kind: Deployment
            name: cert-manager
            namespace: cert-manager
            wait: true
            wait_condition:
              type: Available
              status: "True"
            wait_timeout: 300
            
        - name: Wait for Cert-Manager webhook to be ready
          k8s_info:
            api_version: apps/v1
            kind: Deployment
            name: cert-manager-webhook
            namespace: cert-manager
            wait: true
            wait_condition:
              type: Available
              status: "True"
            wait_timeout: 180
            
        - name: Test Cert-Manager webhook with test certificate
          k8s:
            definition:
              apiVersion: cert-manager.io/v1
              kind: Certificate
              metadata:
                name: test-selfsigned
                namespace: cert-manager
              spec:
                secretName: test-selfsigned-tls
                issuerRef:
                  name: test-selfsigned
                  kind: ClusterIssuer
                commonName: test.local
                dnsNames:
                  - test.local
            wait: true
            wait_condition:
              type: Ready
              status: "True"
            wait_timeout: 120
          register: cert_test
          
        - name: Clean up test certificate
          k8s:
            api_version: cert-manager.io/v1
            kind: Certificate
            name: test-selfsigned
            namespace: cert-manager
            state: absent
            
        - name: Phase 4 Complete - Cert-Manager is healthy
          debug:
            msg: "‚úÖ Cert-Manager deployed and verified healthy"

    # ===========================================
    # PHASE 5: STORAGE PROVISIONING
    # ===========================================
    - name: Phase 5a - Deploy Local Storage Provisioner
      block:
        - name: Check if Local Path Provisioner is already installed
          k8s_info:
            api_version: apps/v1
            kind: Deployment
            name: local-path-provisioner
            namespace: local-path-storage
          register: local_path_check
          
        - name: Add Rancher Local Path Provisioner repository
          kubernetes.core.helm_repository:
            name: local-path-provisioner
            repo_url: https://charts.rancher.io
          when: local_path_check.resources | length == 0
          
        - name: Deploy Local Path Provisioner
          kubernetes.core.helm:
            name: local-path-provisioner
            chart_ref: local-path-provisioner/local-path-provisioner
            release_namespace: local-path-storage
            create_namespace: true
            values:
              image:
                repository: rancher/local-path-provisioner
                tag: v0.0.24
              storageClass:
                create: true
                defaultClass: true
                name: local-path
                reclaimPolicy: Retain
              nodePathMap:
                - node: DEFAULT_PATH_FOR_NON_LISTED_NODES
                  paths:
                    - /data/local-path-provisioner
              tolerations:
                - key: node-role.kubernetes.io/control-plane
                  operator: Equal
                  effect: NoSchedule
          when: local_path_check.resources | length == 0
          
        - name: Wait for Local Path Provisioner to be ready
          k8s_info:
            api_version: apps/v1
            kind: Deployment
            name: local-path-provisioner
            namespace: local-path-storage
            wait: true
            wait_condition:
              type: Available
              status: "True"
            wait_timeout: 180

    # ===========================================
    # PHASE 5b: VAULT (SECRETS MANAGEMENT)
    # ===========================================
    - name: Phase 5b - Deploy HashiCorp Vault with Storage
      block:
        - name: Check if Vault is already installed
          k8s_info:
            api_version: apps/v1
            kind: StatefulSet
            name: vault
            namespace: vault
          register: vault_check
          
        - name: Add HashiCorp Helm repository
          kubernetes.core.helm_repository:
            name: hashicorp
            repo_url: https://helm.releases.hashicorp.com
          when: vault_check.resources | length == 0
          
        - name: Deploy Vault with HA and persistent storage
          kubernetes.core.helm:
            name: vault
            chart_ref: hashicorp/vault
            release_namespace: vault
            create_namespace: true
            chart_version: "{{ vault_version }}"
            values:
              global:
                enabled: true
                tlsDisable: true
              injector:
                enabled: true
                replicas: 2
                resources:
                  requests:
                    memory: 256Mi
                    cpu: 250m
                  limits:
                    memory: 256Mi
                    cpu: 250m
              server:
                image:
                  repository: hashicorp/vault
                  tag: "1.15.2"
                  pullPolicy: IfNotPresent
                resources:
                  requests:
                    memory: 256Mi
                    cpu: 250m
                  limits:
                    memory: 256Mi
                    cpu: 250m
                readinessProbe:
                  enabled: true
                  path: "/v1/sys/health?standbyok=true&sealedcode=204&uninitcode=204"
                livenessProbe:
                  enabled: true
                  path: "/v1/sys/health?standbyok=true"
                  initialDelaySeconds: 60
                postStart: []
                extraInitContainers: null
                extraContainers: null
                shareProcessNamespace: false
                extraArgs: ""
                extraPorts: null
                extraEnvironmentVars: {}
                extraSecretEnvironmentVars: []
                extraVolumes: []
                volumes: null
                volumeMounts: null
                affinity: |
                  podAntiAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                      - labelSelector:
                          matchLabels:
                            app.kubernetes.io/name: {{ template "vault.name" . }}
                            app.kubernetes.io/instance: "{{ .Release.Name }}"
                            component: server
                        topologyKey: kubernetes.io/hostname
                tolerations: []
                nodeSelector: {}
                networkPolicy:
                  enabled: false
                priorityClassName: ""
                extraLabels: {}
                annotations: {}
                service:
                  enabled: true
                  clusterIP: null
                  type: ClusterIP
                  nodePort: null
                  port: 8200
                  targetPort: 8200
                  annotations: {}
                dataStorage:
                  enabled: true
                  size: 10Gi
                  mountPath: "/vault/data"
                  storageClass: local-path
                  accessMode: ReadWriteOnce
                  annotations: {}
                auditStorage:
                  enabled: true
                  size: 5Gi
                  mountPath: "/vault/audit"
                  storageClass: local-path
                  accessMode: ReadWriteOnce
                  annotations: {}
                dev:
                  enabled: false
                standalone:
                  enabled: false
                ha:
                  enabled: true
                  replicas: 3
                  raft:
                    enabled: true
                    setNodeId: true
                    config: |
                      ui = true
                      
                      listener "tcp" {
                        tls_disable = 1
                        address = "[::]:8200"
                        cluster_address = "[::]:8201"
                        # Enable unauthenticated metrics access (necessary for Prometheus)
                        telemetry {
                          unauthenticated_metrics_access = "true"
                        }
                      }
                      
                      storage "raft" {
                        path = "/vault/data"
                        
                        retry_join {
                          leader_api_addr = "http://vault-0.vault-internal:8200"
                        }
                        
                        retry_join {
                          leader_api_addr = "http://vault-1.vault-internal:8200"
                        }
                        
                        retry_join {
                          leader_api_addr = "http://vault-2.vault-internal:8200"
                        }
                        
                        autopilot {
                          cleanup_dead_servers = "true"
                          last_contact_threshold = "200ms"
                          last_contact_failure_threshold = "10m"
                          max_trailing_logs = 250000
                          min_quorum = 5
                          server_stabilization_time = "10s"
                        }
                      }
                      
                      service_registration "kubernetes" {}
              ui:
                enabled: true
                serviceType: "ClusterIP"
                serviceNodePort: null
                externalPort: 8200
                targetPort: 8200
          when: vault_check.resources | length == 0
          
        - name: Wait for Vault StatefulSet to be ready
          k8s_info:
            api_version: apps/v1
            kind: StatefulSet
            name: vault
            namespace: vault
            wait: true
            wait_condition:
              type: Ready
              status: "True"
            wait_timeout: 600
            
        - name: Verify Vault pods are running
          k8s_info:
            api_version: v1
            kind: Pod
            namespace: vault
            label_selectors:
              - "app.kubernetes.io/name=vault"
            field_selectors:
              - "status.phase=Running"
          register: vault_pods
          until: vault_pods.resources | length >= 3
          retries: 30
          delay: 10
            
        - name: Phase 5 Complete - Vault with storage is deployed
          debug:
            msg: 
              - "‚úÖ Local Path Provisioner deployed successfully"
              - "‚úÖ Vault deployed with HA and persistent storage"
              - "üìä Vault status: {{ vault_pods.resources | length }}/3 pods running"
              - ""
              - "‚ö†Ô∏è  Manual initialization required:"
              - "1. kubectl exec vault-0 -n vault -- vault operator init"
              - "2. kubectl exec vault-0 -n vault -- vault operator unseal"
              - "3. Repeat unseal for vault-1 and vault-2 pods"


    # ===========================================
    # FINAL INFRASTRUCTURE HEALTH CHECK
    # ===========================================
    - name: Final Infrastructure Health Check
      block:
        - name: Verify all critical pods are running
          k8s_info:
            api_version: v1
            kind: Pod
            namespace: "{{ item }}"
            field_selectors:
              - "status.phase=Running"
          register: running_pods
          loop:
            - kube-system
            - ingress-nginx
            - cert-manager
            - vault
            - local-path-storage
          
        - name: Display infrastructure deployment summary
          debug:
            msg:
              - "üéâ CRITICAL INFRASTRUCTURE DEPLOYMENT COMPLETE!"
              - ""
              - "‚úÖ Cilium CNI: Deployed and healthy"
              - "‚úÖ Multus CNI: Deployed and healthy"  
              - "‚úÖ NGINX Ingress: Deployed and healthy"
              - "‚úÖ Cert-Manager: Deployed and healthy"
              - "‚úÖ Local Path Provisioner: Deployed and healthy"
              - "‚úÖ Vault: Deployed with HA and persistent storage"
              - ""
              - "üîß CLUSTER STATUS:"
              - "‚Ä¢ Pod networking: Operational"
              - "‚Ä¢ HTTP/HTTPS routing: Ready"
              - "‚Ä¢ TLS certificate automation: Ready"
              - "‚Ä¢ Persistent storage: Available"
              - "‚Ä¢ Secrets management: Ready (requires init)"
              - ""
              - "Next phase: Deploy ArgoCD for GitOps management"

  handlers:
    - name: restart containerd
      systemd:
        name: containerd
        state: restarted
        daemon_reload: true