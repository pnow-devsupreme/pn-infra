---
# Global configuration
global:
  repoURL: 'https://github.com/pnow-devsupreme/pn-infra.git'
  targetRevision: 'main'

# Labels configuration
labels:
  component: storage
  managedBy: argocd

# Ceph Cluster Configuration
# Based on official rook-ceph-cluster v1.18.4 chart

# Operator namespace
operatorNamespace: rook-ceph

# Cluster Configuration
cephClusterSpec:
  # Ceph version
  cephVersion:
    image: quay.io/ceph/ceph:v18.2.4
    allowUnsupported: false

  # Data directory on host
  dataDirHostPath: /var/lib/rook

  # Skip upgrade checks
  skipUpgradeChecks: false

  # Continue if PGs are not clean during upgrade
  continueUpgradeAfterChecksEvenIfNotHealthy: false

  # Wait timeout if PGs are not clean (in minutes)
  waitTimeoutForHealthyOSDInMinutes: 10

  # Monitor configuration
  mon:
    count: 3  # Deploy 3 monitors for quorum
    allowMultiplePerNode: false  # Don't run multiple mons on same node
    # Use hostPath instead of PVCs for monitor data
    # volumeClaimTemplate: null  # Disabled - using dataDirHostPath instead

  # Manager configuration
  mgr:
    count: 2 # Run 2 managers for HA
    allowMultiplePerNode: false
    modules:
    - name: pg_autoscaler
      enabled: true
    - name: rook
      enabled: true

  # Dashboard configuration
  dashboard:
    enabled: false
    ssl: true
    port: 8443

  # Monitoring configuration
  monitoring:
    enabled: false
    createPrometheusRules: false # We'll create these when prometheus is deployed

  # Network configuration
  network:
    connections:
      encryption:
        enabled: false # Disable for performance in trusted network
      compression:
        enabled: false # Disable for performance
      requireMsgr2: false

  # Crash collector
  crashCollector:
    disable: false

  # Log collector
  logCollector:
    enabled: true
    periodicity: daily
    maxLogSize: 500M

  # Cleanup policy on cluster deletion
  cleanupPolicy:
    confirmation: "" # Must be set to "yes-really-destroy-data" to allow cleanup
    sanitizeDisks:
      method: quick
      dataSource: zero
      iteration: 1
    allowUninstallWithVolumes: false

  # Annotations and labels
  annotations: {}
  labels: {}

  # Placement configuration for Ceph daemons
  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: role
                  operator: In
                  values:
                    - storage-node
                - key: ceph-osd
                  operator: In
                  values:
                    - enabled
      tolerations:
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule

  # Storage Configuration
  storage:
    useAllNodes: false  # Explicitly define which nodes to use
    useAllDevices: false  # Explicitly define which devices to use

    nodes:
      - name: "k8s-master-01"
        devices:
          - name: "/dev/sdb"
      - name: "k8s-master-02"
        devices:
          - name: "/dev/sdb"
      - name: "k8s-master-03"
        devices:
          - name: "/dev/sdb"
      - name: "k8s-worker-02"
        devices:
          - name: "/dev/sdb"

  # Remove OSDs that are not in the list above
  removeOSDsIfOutAndSafeToRemove: false

  # Priority class for pods
  priorityClassNames:
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical

  # Disruption budgets
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0
    manageMachineDisruptionBudgets: false
    machineDisruptionBudgetNamespace: openshift-machine-api

  # Resource limits/requests for Ceph daemons
  resources:
    mon:
      limits:
        cpu: "2000m"
        memory: "2Gi"
      requests:
        cpu: "500m"
        memory: "1Gi"
    osd:
      limits:
        cpu: "2000m"
        memory: "4Gi"
      requests:
        cpu: "1000m"
        memory: "2Gi"
    mgr:
      limits:
        cpu: "1000m"
        memory: "1Gi"
      requests:
        cpu: "500m"
        memory: "512Mi"
    prepareosd:
      requests:
        cpu: "500m"
        memory: "50Mi"
    crashcollector:
      limits:
        cpu: "500m"
        memory: "60Mi"
      requests:
        cpu: "100m"
        memory: "60Mi"
    logcollector:
      limits:
        cpu: "500m"
        memory: "1Gi"
      requests:
        cpu: "100m"
        memory: "100Mi"
    cleanup:
      limits:
        cpu: "500m"
        memory: "1Gi"
      requests:
        cpu: "100m"
        memory: "100Mi"

# Ingress for Ceph Dashboard
ingress:
  dashboard:
    enabled: false # Enable after cluster is healthy
    host:
      name: ceph.pnats.cloud
    ingressClassName: nginx
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-production
      nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
      nginx.ingress.kubernetes.io/ssl-passthrough: "true"
    tls:
    - secretName: ceph-dashboard-tls
      hosts:
      - ceph.pnats.cloud

# CephBlockPool for RBD
cephBlockPools:
- name: replicapool
  spec:
    failureDomain: host # Replicate across hosts
    replicated:
      size: 3 # 3 replicas (matches number of OSDs)
      requireSafeReplicaSize: true
  storageClass:
    enabled: true
    name: ceph-block
    isDefault: false
    reclaimPolicy: Delete
    allowVolumeExpansion: true
    volumeBindingMode: Immediate
    mountOptions: []
    parameters:
      imageFormat: "2"
      imageFeatures: layering
      csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
      csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
      csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
      csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
      csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
      csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
      csi.storage.k8s.io/fstype: ext4

# CephFilesystem for CephFS
cephFileSystems:
- name: cephfs
  spec:
    metadataPool:
      replicated:
        size: 3
    dataPools:
    - name: data0
      failureDomain: host
      replicated:
        size: 3
    metadataServer:
      activeCount: 1
      activeStandby: true
      resources:
        limits:
          cpu: "1000m"
          memory: "4Gi"
        requests:
          cpu: "500m"
          memory: "1Gi"
      priorityClassName: system-cluster-critical
  storageClass:
    enabled: true
    name: ceph-filesystem
    isDefault: false
    reclaimPolicy: Delete
    allowVolumeExpansion: true
    volumeBindingMode: Immediate
    mountOptions: []
    parameters:
      csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
      csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
      csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
      csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
      csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
      csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
      csi.storage.k8s.io/fstype: ext4

# CephObjectStore for RGW (S3/Swift)
cephObjectStores:
- name: objectstore
  spec:
    metadataPool:
      failureDomain: host
      replicated:
        size: 3
    dataPool:
      failureDomain: host
      replicated:
        size: 3 # Using replicated instead of erasure coding for 3-node cluster
    preservePoolsOnDelete: true
    gateway:
      port: 80
      securePort: 443
      instances: 2 # Run 2 RGW instances for HA
      priorityClassName: system-cluster-critical
      resources:
        limits:
          cpu: "1000m"
          memory: "2Gi"
        requests:
          cpu: "500m"
          memory: "1Gi"
    healthCheck:
      bucket:
        interval: 60s
  storageClass:
    enabled: true
    name: ceph-bucket
    reclaimPolicy: Delete
    parameters:
      region: ap-south-2
