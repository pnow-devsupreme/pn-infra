apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: rook-ceph-cluster
  namespace: argocd
spec:
  project: default
  source:
    repoURL: "https://charts.rook.io/release"
    targetRevision: v1.15.7
    chart: rook-ceph-cluster
    helm:
      valuesObject:
        # Installs a debugging toolbox deployment
        toolbox:
          enabled: true
          containerSecurityContext:
            runAsNonRoot: true
            runAsUser: 2016
            runAsGroup: 2016
            capabilities:
              drop: ["ALL"]
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                      - key: kubernetes.io/hostname
                        operator: In
                        values:
                          - pn-hyd-wr02-1a
          tolerations:
            - key: node-role.kubernetes.io/control-plane
              operator: Exists
              effect: NoSchedule
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
        operatorNamespace: rook-ceph
        clusterName: rook-ceph-cluster
        monitoring:
          # -- Enable Prometheus integration, will also create necessary RBAC rules to allow Operator to create ServiceMonitors.
          # Monitoring requires Prometheus to be pre-installed
          enabled: false
          # -- Whether to disable the metrics reported by Ceph. If false, the prometheus mgr module and Ceph exporter are enabled
          metricsDisabled: false
          # -- Whether to create the Prometheus rules for Ceph alerts
          createPrometheusRules: false
          # -- The namespace in which to create the prometheus rules, if different from the rook cluster namespace.
          # If you have multiple rook-ceph clusters in the same k8s cluster, choose the same namespace (ideally, namespace with prometheus
          # deployed) to set rulesNamespaceOverride for all the clusters. Otherwise, you will get duplicate alerts with multiple alert definitions.
          rulesNamespaceOverride:
          # Monitoring settings for external clusters:
          # externalMgrEndpoints: <list of endpoints>
          # externalMgrPrometheusPort: <port>
          # Scrape interval for prometheus
          # interval: 10s
          # allow adding custom labels and annotations to the prometheus rule
          prometheusRule:
            # -- Labels applied to PrometheusRule
            labels: {}
            # -- Annotations applied to PrometheusRule
            annotations: {}

        # Main Ceph cluster specification
        cephClusterSpec:
          cephVersion:
            image: quay.io/ceph/ceph:v19.2.0
            allowUnsupported: false
          dataDirHostPath: /var/lib/rook
          # Currently Single node configuration
          mon:
            count: 10
            allowMultiplePerNode: true
          mgr:
            count: 10
            allowMultiplePerNode: true
          dashboard:
            enabled: true
            ssl: false
            port: 7000
          network:
            provider: host

          # Placement for single node
          placement:
            all:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                    - matchExpressions:
                        - key: kubernetes.io/hostname
                          operator: In
                          values:
                            - pn-hyd-wr*
                            - pn-hyd-ms01-1a
              tolerations:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
                  effect: NoSchedule
            crashcollector:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                    - matchExpressions:
                        - key: kubernetes.io/hostname
                          operator: In
                          values:
                            - pn-hyd-ms01-1a
              tolerations:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
                  effect: NoSchedule
            # Exporter specific placement
            exporter:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                    - matchExpressions:
                        - key: kubernetes.io/hostname
                          operator: In
                          values:
                            - pn-hyd-wr*
              tolerations:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
                  effect: NoSchedule
            # OSDs placement
            osd:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                    - matchExpressions:
                        - key: kubernetes.io/hostname
                          operator: In
                          values:
                            - pn-hyd-ms01-1a
                            - pn-hyd-wr02-1a
              tolerations:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
                  effect: NoSchedule
            # MDS specific placement
            mds:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                    - matchExpressions:
                        - key: kubernetes.io/hostname
                          operator: In
                          values:
                            - pn-hyd-ms01-1a
                            - pn-hyd-wr02-1a
              tolerations:
                - key: node-role.kubernetes.io/control-plane
                  operator: Exists
                  effect: NoSchedule
          # Resource configuration
          resources:
            mgr:
              limits:
                cpu: 2
                memory: 2Gi
              requests:
                cpu: 500m
                memory: 512Mi
            mon:
              limits:
                cpu: 4
                memory: 2Gi
              requests:
                cpu: 1
                memory: 1Gi
            osd:
              limits:
                cpu: 4
                memory: 3Gi
              requests:
                cpu: 2
                memory: 1500Mi
            mgr-sidecar:
              limits:
                cpu: 200m
                memory: 200Mi
              requests:
                cpu: 100m
                memory: 100Mi
            crashcollector:
              limits:
                cpu: 200m
                memory: 60Mi
              requests:
                cpu: 100m
                memory: 60Mi
            logcollector:
              limits:
                cpu: 200m
                memory: 1Gi
              requests:
                cpu: 100m
                memory: 100Mi
            cleanup:
              limits:
                cpu: 1
                memory: 1Gi
              requests:
                cpu: 500m
                memory: 100Mi
            exporter:
              limits:
                cpu: 200m
                memory: 128Mi
              requests:
                cpu: 100m
                memory: 50Mi
          # priority classes to apply to ceph resources
          priorityClassNames:
            mon: system-node-critical
            osd: system-node-critical
            mgr: system-cluster-critical
          # Storage configuration
          storage:
            useAllNodes: false
            useAllDevices: false
            config:
              osdsPerDevice: "4" # this value can be overridden at the node or device level
            nodes:
              - name: "pn-hyd-ms01-1a"
                devices:
                  # First disk - sdb partitions
                  - name: "/dev/sdb1"
                  - name: "/dev/sdb2"
                  - name: "/dev/sdb3"
                  - name: "/dev/sdb4"
                  # Second disk - sdc partitions
                  - name: "/dev/sdc1"
                  - name: "/dev/sdc2"
                  - name: "/dev/sdc3"
                  - name: "/dev/sdc4"

              - name: "pn-hyd-ms02-1a"
                devices:
                  - name: "/dev/sda3"
              - name: "pn-hyd-wr02-1a"
                devices:
                  - name: "/dev/sda2"
                  - name: "/dev/sda3"
          # Configure the healthcheck and liveness probes for ceph pods.
          # Valid values for daemons are 'mon', 'osd', 'status'
          healthCheck:
            daemonHealth:
              mon:
                disabled: false
                interval: 25s
              osd:
                disabled: false
                interval: 30s
              status:
                disabled: false
                interval: 30s
            # Change pod liveness probe, it works for all mon, mgr, and osd pods.
            livenessProbe:
              mon:
                disabled: false
              mgr:
                disabled: false
              osd:
                disabled: false
        ingress:
          dashboard:
            enabled: true
            annotations:
              cert-manager.io/cluster-issuer: letsencrypt-prod
              nginx.ingress.kubernetes.io/backend-protocol: HTTP
              nginx.ingress.kubernetes.io/ssl-redirect: true
              nginx.ingress.kubernetes.io/ssl-verify: false
            ingressClassName: nginx
            host:
              name: ceph.pnats.cloud
              path: /
              pathType: Prefix
            tls:
              - hosts:
                  - ceph.pnats.cloud
                secretName: ceph-dashboard-tls
        # Storage Pools Configuration
        cephBlockPools:
          - name: ceph-block-replicapool
            spec:
              failureDomain: host
              replicated:
                size: 3
            storageClass:
              enabled: true
              name: ceph-block-storage-pool
              isDefault: true
              reclaimPolicy: Delete
              allowVolumeExpansion: true
              volumeBindingMode: Immediate
              parameters:
                imageFormat: "2"
                imageFeatures: layering
                csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
                csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
                csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
                csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
                csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
                csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
                csi.storage.k8s.io/fstype: ext4
        cephFileSystems:
          - name: ceph-fs
            spec:
              metadataPool:
                replicated:
                  size: 3
              dataPools:
                - name: data0
                  replicated:
                    size: 3
              metadataServer:
                activeCount: 1
                activeStandby: true
                resources:
                  limits:
                    cpu: 4
                    memory: 4Gi
                  requests:
                    cpu: 1000m
                    memory: 4Gi
                priorityClassName: system-cluster-critical
            storageClass:
              enabled: true
              isDefault: false
              name: ceph-filesystem
              pool: data0
              reclaimPolicy: Delete
              allowVolumeExpansion: true
              volumeBindingMode: "Immediate"
              parameters:
                csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
                csi.storage.k8s.io/provisioner-secret-namespace: "rook-ceph"
                csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
                csi.storage.k8s.io/controller-expand-secret-namespace: "rook-ceph"
                csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
                csi.storage.k8s.io/node-stage-secret-namespace: "rook-ceph"
        cephFileSystemVolumeSnapshotClass:
          enabled: true
          name: ceph-filesystem-volume-snapshot-class
          isDefault: true
          deletionPolicy: Delete
          annotations: {}
          labels: {}
          # see https://rook.io/docs/rook/v1.10/Storage-Configuration/Ceph-CSI/ceph-csi-snapshot/#cephfs-snapshots for available configuration
          parameters: {}

        # -- Settings for the block pool snapshot class
        # @default -- See [RBD Snapshots](../Storage-Configuration/Ceph-CSI/ceph-csi-snapshot.md#rbd-snapshots)
        cephBlockPoolsVolumeSnapshotClass:
          enabled: true
          name: ceph-block-pool-volume-snapshot-class
          isDefault: true
          deletionPolicy: Delete
          annotations: {}
          labels: {}
          # see https://rook.io/docs/rook/v1.10/Storage-Configuration/Ceph-CSI/ceph-csi-snapshot/#rbd-snapshots for available configuration
          parameters: {}
        # cephObjectStores:
        #   - name: ceph-objectstore
        #     # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Object-Storage/ceph-object-store-crd.md#object-store-settings for available configuration
        #     spec:
        #       metadataPool:
        #         failureDomain: osd
        #         replicated:
        #           size: 1
        #       dataPool:
        #         failureDomain: osd
        #         erasureCoded:
        #           dataChunks: 2
        #           codingChunks: 1
        #         parameters:
        #           bulk: "true"
        #       protocols:
        #         s3:
        #           enabled: true
        #       preservePoolsOnDelete: true
        #       gateway:
        #         port: 8090
        #         resources:
        #           limits:
        #             memory: "2Gi"
        #           requests:
        #             cpu: "1000m"
        #             memory: "1Gi"
        #         # securePort: 443
        #         # sslCertificateRef:
        #         instances: 1
        #         priorityClassName: system-cluster-critical
        #         # opsLogSidecar:
        #         #   resources:
        #         #     limits:
        #         #       memory: "100Mi"
        #         #     requests:
        #         #       cpu: "100m"
        #         #       memory: "40Mi"
        #     storageClass:
        #       enabled: true
        #       name: ceph-bucket
        #       reclaimPolicy: Delete
        #       volumeBindingMode: "Immediate"
        #       annotations: {}
        #       labels: {}
        #       # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Object-Storage-RGW/ceph-object-bucket-claim.md#storageclass for available configuration
        #       parameters:
        #         # note: objectStoreNamespace and objectStoreName are configured by the chart
        #         region: ap-south-1
        #     ingress:
        #       # Enable an ingress for the ceph-objectstore
        #       enabled: true
        #       annotations:
        #         cert-manager.io/cluster-issuer: letsencrypt-prod
        #         nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
        #         nginx.ingress.kubernetes.io/ssl-redirect: "true"
        #         nginx.ingress.kubernetes.io/ssl-verify: "false"
        #       ingressClassName: nginx
        #       host:
        #         name: s3.proficientnowtech.com
        #         path: /
        #         pathType: Prefix
        #       tls:
        #         - hosts:
        #             - s3.proficientnowtech.com
        #           secretName: ceph-objectstore-tls
      skipCrds: false
  destination:
    server: https://kubernetes.default.svc
    namespace: rook-ceph
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
      - ServerSideApply=true
